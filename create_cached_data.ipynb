{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths for caching\n",
    "PAPERS_FILE = \"cached_papers.json\"\n",
    "EMBEDDINGS_FILE = \"cached_embeddings.npy\"\n",
    "\n",
    "# Function to fetch all available papers from ArXiv for a given category\n",
    "def fetch_all_arxiv_papers(category, batch_size=100):\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    start = 0\n",
    "    all_papers = []\n",
    "\n",
    "    print(f\"Fetching papers for category: {category}...\")\n",
    "\n",
    "    while True:\n",
    "        search_query = f\"search_query=cat:{category}&start={start}&max_results={batch_size}\"\n",
    "        response = requests.get(base_url + search_query)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch data for {category}\")\n",
    "            break\n",
    "\n",
    "        root = ET.fromstring(response.content)\n",
    "        papers = []\n",
    "\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            abstract = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "            link = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "            papers.append({\"title\": title, \"abstract\": abstract, \"link\": link})\n",
    "\n",
    "        if not papers:\n",
    "            print(f\"Finished fetching papers for {category}. Total: {len(all_papers)}\")\n",
    "            break\n",
    "\n",
    "        all_papers.extend(papers)\n",
    "        start += batch_size\n",
    "\n",
    "    return all_papers\n",
    "\n",
    "# Load Pre-trained SPECTER Model\n",
    "model_name = \"allenai/specter\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Function to Generate Embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "\n",
    "    return output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Load cached data if available\n",
    "if os.path.exists(PAPERS_FILE) and os.path.exists(EMBEDDINGS_FILE):\n",
    "    print(\"Cached data already exists.\")\n",
    "else:\n",
    "    print(\"Fetching data from ArXiv...\")\n",
    "    # Categories: AI, ML, and NLP (ArXiv categories)\n",
    "    categories = [\"cs.AI\", \"cs.LG\", \"cs.CL\"]\n",
    "\n",
    "    # Fetch all papers for each category\n",
    "    all_papers = []\n",
    "    for category in categories:\n",
    "        all_papers.extend(fetch_all_arxiv_papers(category))\n",
    "\n",
    "    print(f\"Total papers fetched: {len(all_papers)}\")\n",
    "\n",
    "    # Save papers to JSON\n",
    "    with open(PAPERS_FILE, \"w\") as f:\n",
    "        json.dump(all_papers, f)\n",
    "\n",
    "    # Convert fetched papers into text format\n",
    "    papers_text = [f\"{paper['title']}: {paper['abstract']}\" for paper in all_papers]\n",
    "\n",
    "    # Generate Paper Embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    paper_embeddings = np.array([get_embedding(paper) for paper in papers_text]).squeeze()\n",
    "\n",
    "    # Save embeddings to file\n",
    "    np.save(EMBEDDINGS_FILE, paper_embeddings)\n",
    "\n",
    "print(\"Data creation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}