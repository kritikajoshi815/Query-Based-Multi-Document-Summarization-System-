{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu7GIDV3fbcp",
        "outputId": "cae80157-50ec-47e4-9d58-5f7baf21ab1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Scientific Paper Query and Summarization System\n",
            "==============================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n",
            "\n",
            "Enter your research query: Transformer models for machine translation\n",
            "\n",
            "Available models: Pegasus, BART, Pegasus_arxiv\n",
            "Enter the model to use (Pegasus, BART, or Pegasus_arxiv): bart\n",
            "\n",
            "Processing your query. This may take a few minutes...\n",
            "\n",
            "Loaded 7068 papers and embeddings\n",
            "Loading Specter model...\n",
            "Loading fine-tuned BART model...\n",
            "Generating query embedding...\n",
            "Found 7068 papers, processing top 5...\n",
            "\n",
            "Top 5 relevant papers:\n",
            "1. Six Challenges for Neural Machine Translation\n",
            "   http://arxiv.org/abs/1706.03872v1\n",
            "\n",
            "2. OpenNMT: Open-source Toolkit for Neural Machine Translation\n",
            "   http://arxiv.org/abs/1709.03815v1\n",
            "\n",
            "3. Neural Machine Translation\n",
            "   http://arxiv.org/abs/1709.07809v1\n",
            "\n",
            "4. SentEval: An Evaluation Toolkit for Universal Sentence Representations\n",
            "   http://arxiv.org/abs/1803.05449v1\n",
            "\n",
            "5. Adapting Sequence Models for Sentence Correction\n",
            "   http://arxiv.org/abs/1707.09067v1\n",
            "\n",
            "Extracting relevant content from papers...\n",
            "Processing paper 1/5: Six Challenges for Neural Machine Translation\n",
            "Processing paper 2/5: OpenNMT: Open-source Toolkit for Neural Machine Translation\n",
            "Processing paper 3/5: Neural Machine Translation\n",
            "Processing paper 4/5: SentEval: An Evaluation Toolkit for Universal Sentence Representations\n",
            "Processing paper 5/5: Adapting Sequence Models for Sentence Correction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`generation_config` default values have been modified to match model-specific defaults: {'min_length': 56, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2, 'pad_token_id': 1, 'bos_token_id': 0, 'eos_token_id': 2, 'decoder_start_token_id': 2}. If this is not desired, please set these values explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating summary...\n",
            "Input text length: 27165 characters\n",
            "Tokenization successful\n",
            "\n",
            "=== SUMMARY ===\n",
            "Research shows that neural machine translation (NMT) systems, like those trained on large data sets, struggle with poor performance on rare word translation. This is because the training data leads these systems to decide on specific word choices during decoding, buried in large matrices of values. NMT systems, however, outperform SMT systems on translation of very infrequent words, particularly those belonging to flected categories. These include Chinese English, German English, and Russian English, where NMT outperforms SMT by a wide margin, particularly on rare words. While NMT improves on SMT on these rare words, it still struggles on rare ones, particularly in domains like Subtitles and German English. This highlights the need to develop better analytics for NMT, particularly for rare words like Chinese English and Russian.\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install datasets\n",
        "!pip install rouge-score\n",
        "!pip install bert-score\n",
        "!pip install transformers\n",
        "\n",
        "import nltk\n",
        "# Download all necessary NLTK resources explicitly\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Add this missing resource\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import PyPDF2\n",
        "import xml.etree.ElementTree as ET\n",
        "from bs4 import BeautifulSoup\n",
        "from io import BytesIO\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer, AutoModel\n",
        "from transformers import AutoTokenizer as BertTokenizer, AutoModel as BertModel\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Add explicit download for missing resource\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# File paths\n",
        "DRIVE_PATH = '/content/drive/MyDrive/nlp_project'\n",
        "PAPERS_FILE = os.path.join(DRIVE_PATH, \"cached_papers.json\")\n",
        "EMBEDDINGS_FILE = os.path.join(DRIVE_PATH, \"cached_embeddings.npy\")\n",
        "PEGASUS_MODEL_PATH = os.path.join(DRIVE_PATH, \"pegasus_finetuned_model\")\n",
        "BART_MODEL_PATH = os.path.join(DRIVE_PATH, \"bart_finetuned_model\")\n",
        "PEGASUS_ARXIV_MODEL_PATH = os.path.join(DRIVE_PATH, \"pegasus_arxiv_finetuned_model\")\n",
        "PEGASUS_ARXIV_FALLBACK_MODEL = \"google/pegasus-arxiv\"\n",
        "\n",
        "def clean_extracted_text(text):\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Basic URL removal\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove math equations completely\n",
        "    text = re.sub(r'\\$[^$]*\\$|\\$\\$[^$]*\\$\\$', '', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9]+[=><+\\-*/^()[\\]{}]+[a-zA-Z0-9]+', '', text)\n",
        "\n",
        "    # Remove references section\n",
        "    ref_patterns = [\n",
        "        r'References\\s*\\n', r'REFERENCES\\s*\\n', r'Bibliography\\s*\\n',\n",
        "        r'BIBLIOGRAPHY\\s*\\n', r'Works Cited\\s*\\n', r'REFERENCES CITED\\s*\\n'\n",
        "    ]\n",
        "    ref_start = len(text)\n",
        "    for pattern in ref_patterns:\n",
        "        matches = list(re.finditer(pattern, text))\n",
        "        if matches:\n",
        "            ref_start = min(ref_start, matches[-1].start())\n",
        "    text = text[:ref_start]\n",
        "\n",
        "    # Fix ligatures and special characters\n",
        "    ligature_map = {\n",
        "        '\\ufb01': 'fi', '\\ufb02': 'fl', '\\ufb00': 'ff', '\\ufb03': 'ffi', '\\ufb04': 'ffl',\n",
        "        '\\u2019': \"'\", '\\u2018': \"'\", '\\u201c': '\"', '\\u201d': '\"', '\\u2014': '-', '\\u2013': '-', '\\u0003': ''\n",
        "    }\n",
        "    for ligature, replacement in ligature_map.items():\n",
        "        text = text.replace(ligature, replacement)\n",
        "\n",
        "    # Remove specific patterns like dates, numbers, citations\n",
        "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{2,4}[/-]\\d{1,2}[/-]\\d{1,2}\\b', '', text)\n",
        "    text = re.sub(r'\\b\\d+\\.\\d+%?\\b|\\b\\d{2,}\\b|\\[\\d+\\]|\\(\\d+\\)', '', text)\n",
        "\n",
        "    # Fix hyphenated words across lines\n",
        "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
        "\n",
        "    # Add spaces between joined different case words\n",
        "    text = re.sub(r'([a-z])([A-Z])|([a-z])([0-9])|([0-9])([a-z])', r'\\1 \\2', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Fix spacing around punctuation\n",
        "    text = re.sub(r'\\s+([.,;:!?)])', r'\\1', text)\n",
        "    text = re.sub(r'([.,;:!?])([a-zA-Z])', r'\\1 \\2', text)\n",
        "\n",
        "    # Remove any remaining special characters\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?()\\-\\'\\\"]+', ' ', text)\n",
        "\n",
        "    # Final whitespace cleanup\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def download_arxiv_paper(arxiv_id):\n",
        "    if \"arxiv.org\" in arxiv_id:\n",
        "        arxiv_id = arxiv_id.split('/')[-1].split('v')[0]\n",
        "\n",
        "    api_url = f\"http://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
        "    response = requests.get(api_url)\n",
        "    soup = BeautifulSoup(response.content, 'xml')\n",
        "    abstract = soup.find('summary').text.strip() if soup.find('summary') else \"\"\n",
        "    title = soup.find('title').text.strip() if soup.find('title') else \"\"\n",
        "\n",
        "    pdf_url = f\"http://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "    response = requests.get(pdf_url)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            pdf_file = BytesIO(response.content)\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "            full_text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                full_text += page.extract_text()\n",
        "\n",
        "            cleaned_text = clean_extracted_text(full_text)\n",
        "\n",
        "            # Extract introduction\n",
        "            introduction = \"\"\n",
        "            intro_patterns = [\n",
        "                r\"(?i)(?:1\\.?\\s*|I\\.?\\s*)?Introduction(.*?)(?:\\n\\d\\.|\\n[A-Z]\\.|\\nII\\.)\",\n",
        "                r\"(?i)(?:1\\.?\\s*|I\\.?\\s*)?Introduction(.*?)(?=\\n2\\.|\\nII\\.)\"\n",
        "            ]\n",
        "            for pattern in intro_patterns:\n",
        "                intro_match = re.search(pattern, cleaned_text, re.DOTALL)\n",
        "                if intro_match:\n",
        "                    introduction = clean_extracted_text(intro_match.group(1).strip())\n",
        "                    break\n",
        "\n",
        "            # Extract conclusion\n",
        "            conclusion = \"\"\n",
        "            concl_patterns = [\n",
        "                r\"(?i)(?:\\d\\.?\\s*|[IVX]+\\.?\\s*)?Conclusion[s]?(.*?)(?:\\n\\d\\.|\\n[A-Z]\\.|\\nReferences|\\n[IVX]+\\.)\",\n",
        "                r\"(?i)(?:\\d\\.?\\s*|[IVX]+\\.?\\s*)?Discussion(?:s)?(.*?)(?:\\n\\d\\.|\\n[A-Z]\\.|\\nReferences|\\n[IVX]+\\.)\"\n",
        "            ]\n",
        "            for pattern in concl_patterns:\n",
        "                concl_match = re.search(pattern, cleaned_text, re.DOTALL)\n",
        "                if concl_match:\n",
        "                    conclusion = clean_extracted_text(concl_match.group(1).strip())\n",
        "                    break\n",
        "\n",
        "            return {\n",
        "                \"title\": title,\n",
        "                \"abstract\": abstract,\n",
        "                \"introduction\": introduction,\n",
        "                \"conclusion\": conclusion,\n",
        "                \"full_text\": cleaned_text\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from PDF: {e}\")\n",
        "            return {\"title\": title, \"abstract\": abstract, \"introduction\": \"\", \"conclusion\": \"\", \"full_text\": \"\"}\n",
        "\n",
        "    return {\"title\": title, \"abstract\": abstract, \"introduction\": \"\", \"conclusion\": \"\", \"full_text\": \"\"}\n",
        "\n",
        "def get_bert_embeddings(texts, model_name=\"bert-base-uncased\"):\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        embeddings.append(embedding[0])\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "def highlight_query_relevant_sentences(text, query, top_n=10):\n",
        "    text = clean_extracted_text(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    if not sentences:\n",
        "        return []\n",
        "\n",
        "    all_embeddings = get_bert_embeddings(sentences + [query])\n",
        "    sentence_embeddings = all_embeddings[:-1]\n",
        "    query_embedding = all_embeddings[-1]\n",
        "\n",
        "    similarities = cosine_similarity(sentence_embeddings, query_embedding.reshape(1, -1)).flatten()\n",
        "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
        "\n",
        "    return [(sentences[i], float(similarities[i]), i) for i in top_indices]\n",
        "\n",
        "def get_paragraph_context(sentences, index, context_size=2):\n",
        "    start_idx = max(0, index - context_size)\n",
        "    end_idx = min(len(sentences) - 1, index + context_size)\n",
        "    context_paragraph = \" \".join(sentences[start_idx:end_idx + 1])\n",
        "    return clean_extracted_text(context_paragraph)\n",
        "\n",
        "def extract_relevant_content(paper_data, query, max_sentences_per_section=10, context_size=2):\n",
        "    result = {\n",
        "        \"title\": paper_data[\"title\"],\n",
        "        \"abstract\": paper_data[\"abstract\"],\n",
        "        \"relevant_sections\": []\n",
        "    }\n",
        "\n",
        "    sections = {\n",
        "        \"introduction\": paper_data.get(\"introduction\", \"\"),\n",
        "        \"conclusion\": paper_data.get(\"conclusion\", \"\"),\n",
        "        \"full_text\": paper_data.get(\"full_text\", \"\")\n",
        "    }\n",
        "\n",
        "    for section_name, section_text in sections.items():\n",
        "        if not section_text:\n",
        "            continue\n",
        "\n",
        "        cleaned_text = clean_extracted_text(section_text)\n",
        "        all_sentences = sent_tokenize(cleaned_text)\n",
        "        relevant_sentences = highlight_query_relevant_sentences(cleaned_text, query, top_n=max_sentences_per_section)\n",
        "\n",
        "        if relevant_sentences:\n",
        "            processed_results = []\n",
        "            relevant_sentences.sort(key=lambda x: x[2])  # Sort by index\n",
        "            processed_indices = set()\n",
        "\n",
        "            for sent, score, idx in relevant_sentences:\n",
        "                if idx in processed_indices:\n",
        "                    continue\n",
        "\n",
        "                context_paragraph = get_paragraph_context(all_sentences, idx, context_size)\n",
        "\n",
        "                # Mark processed indices\n",
        "                for i in range(max(0, idx - context_size), min(len(all_sentences), idx + context_size + 1)):\n",
        "                    processed_indices.add(i)\n",
        "\n",
        "                processed_results.append({\n",
        "                    \"text\": context_paragraph,\n",
        "                    \"relevance_score\": score,\n",
        "                    \"core_sentence\": sent\n",
        "                })\n",
        "\n",
        "            result[\"relevant_sections\"].append({\n",
        "                \"section_name\": section_name,\n",
        "                \"sentences\": processed_results\n",
        "            })\n",
        "\n",
        "    return result\n",
        "\n",
        "def process_top_papers(query, top_papers, max_papers=5, context_size=2):\n",
        "    results = []\n",
        "\n",
        "    for i, paper in enumerate(top_papers[:max_papers]):\n",
        "        paper_link = paper['link']\n",
        "        print(f\"Processing paper {i+1}/{max_papers}: {paper['title']}\")\n",
        "\n",
        "        paper_data = download_arxiv_paper(paper_link)\n",
        "        relevant_content = extract_relevant_content(paper_data, query, context_size=context_size)\n",
        "\n",
        "        results.append({\n",
        "            \"paper_index\": i+1,\n",
        "            \"title\": paper['title'],\n",
        "            \"link\": paper['link'],\n",
        "            \"extracted_content\": relevant_content\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_embedding(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "\n",
        "    return output.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "def postprocess_summary(summary):\n",
        "    \"\"\"\n",
        "    Additional processing to clean up the summary from symbols and artifacts\n",
        "    \"\"\"\n",
        "    # Remove any formula-like patterns\n",
        "    summary = re.sub(r'[a-zA-Z0-9]+[=><+\\-*/^()[\\]{}]+[a-zA-Z0-9]+', '', summary)\n",
        "\n",
        "    # Remove isolated special characters\n",
        "    summary = re.sub(r'\\s[/=+\\-*:]+\\s', ' ', summary)\n",
        "\n",
        "    # Remove reference citations\n",
        "    summary = re.sub(r'\\[\\d+\\]|\\(\\d+\\)', '', summary)\n",
        "\n",
        "    # Fix spacing issues\n",
        "    summary = re.sub(r'\\s+', ' ', summary)\n",
        "\n",
        "    # Remove any other non-alphanumeric characters except for basic punctuation\n",
        "    summary = re.sub(r'[^\\w\\s.,;:!?()\\-\\'\\\"]+', '', summary)\n",
        "\n",
        "    # Ensure proper spacing after punctuation\n",
        "    summary = re.sub(r'([.,;:!?])([a-zA-Z])', r'\\1 \\2', summary)\n",
        "\n",
        "    # Ensure proper capitalization of sentences\n",
        "    sentences = sent_tokenize(summary)\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip()\n",
        "        if sentence:\n",
        "            if not sentence[0].isupper() and sentence[0].isalpha():\n",
        "                sentence = sentence[0].upper() + sentence[1:]\n",
        "            processed_sentences.append(sentence)\n",
        "\n",
        "    summary = ' '.join(processed_sentences)\n",
        "\n",
        "    return summary.strip()\n",
        "\n",
        "def run_inference(query, model_choice):\n",
        "    # Load cached papers and embeddings\n",
        "    if os.path.exists(PAPERS_FILE) and os.path.exists(EMBEDDINGS_FILE):\n",
        "        with open(PAPERS_FILE, \"r\") as f:\n",
        "            all_papers = json.load(f)\n",
        "        paper_embeddings = np.load(EMBEDDINGS_FILE)\n",
        "        print(f\"Loaded {len(all_papers)} papers and embeddings\")\n",
        "    else:\n",
        "        print(\"Cached data not found. Please run the training script first.\")\n",
        "        return None\n",
        "\n",
        "    # Load Specter model\n",
        "    try:\n",
        "        print(\"Loading Specter model...\")\n",
        "        specter_tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter\")\n",
        "        specter_model = AutoModel.from_pretrained(\"allenai/specter\").to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Specter model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Load selected model\n",
        "    try:\n",
        "        if model_choice.lower() == \"pegasus\":\n",
        "            print(\"Loading fine-tuned Pegasus model...\")\n",
        "            tokenizer = PegasusTokenizer.from_pretrained(PEGASUS_MODEL_PATH)\n",
        "            model = PegasusForConditionalGeneration.from_pretrained(PEGASUS_MODEL_PATH).to(device)\n",
        "            max_input_length = 512\n",
        "            max_output_length = 256\n",
        "            generation_config = GenerationConfig(\n",
        "                num_beams=5,\n",
        "                length_penalty=1.2,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=3,\n",
        "                do_sample=True,\n",
        "                top_p=0.92,\n",
        "                temperature=0.85,\n",
        "                max_length=max_output_length\n",
        "            )\n",
        "            fallback_config = GenerationConfig(\n",
        "                num_beams=8,\n",
        "                length_penalty=1.5,\n",
        "                early_stopping=True,\n",
        "                no_repeat_ngram_size=2,\n",
        "                temperature=0.7,\n",
        "                max_length=max_output_length\n",
        "            )\n",
        "        elif model_choice.lower() == \"bart\":\n",
        "            print(\"Loading fine-tuned BART model...\")\n",
        "            tokenizer = BartTokenizer.from_pretrained(BART_MODEL_PATH)\n",
        "            model = BartForConditionalGeneration.from_pretrained(BART_MODEL_PATH).to(device)\n",
        "            max_input_length = 1024\n",
        "            max_output_length = 256\n",
        "            generation_config = GenerationConfig(\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "                max_length=max_output_length\n",
        "            )\n",
        "            fallback_config = GenerationConfig(\n",
        "                num_beams=6,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "                max_length=max_output_length\n",
        "            )\n",
        "        elif model_choice.lower() == \"pegasus_arxiv\":\n",
        "            print(f\"Attempting to load fine-tuned Pegasus-arxiv model from: {PEGASUS_ARXIV_MODEL_PATH}\")\n",
        "            if os.path.exists(PEGASUS_ARXIV_MODEL_PATH) and os.path.exists(os.path.join(PEGASUS_ARXIV_MODEL_PATH, \"config.json\")):\n",
        "                try:\n",
        "                    tokenizer = PegasusTokenizer.from_pretrained(PEGASUS_ARXIV_MODEL_PATH)\n",
        "                    model = PegasusForConditionalGeneration.from_pretrained(PEGASUS_ARXIV_MODEL_PATH).to(device)\n",
        "                    print(\"Fine-tuned Pegasus-arxiv model loaded successfully\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading fine-tuned model: {e}\")\n",
        "                    print(f\"Falling back to {PEGASUS_ARXIV_FALLBACK_MODEL}\")\n",
        "                    tokenizer = PegasusTokenizer.from_pretrained(PEGASUS_ARXIV_FALLBACK_MODEL)\n",
        "                    model = PegasusForConditionalGeneration.from_pretrained(PEGASUS_ARXIV_FALLBACK_MODEL).to(device)\n",
        "            else:\n",
        "                print(f\"Model directory not found at {PEGASUS_ARXIV_MODEL_PATH}. Falling back to {PEGASUS_ARXIV_FALLBACK_MODEL}\")\n",
        "                tokenizer = PegasusTokenizer.from_pretrained(PEGASUS_ARXIV_FALLBACK_MODEL)\n",
        "                model = PegasusForConditionalGeneration.from_pretrained(PEGASUS_ARXIV_FALLBACK_MODEL).to(device)\n",
        "            max_input_length = 512\n",
        "            max_output_length = 128\n",
        "            generation_config = GenerationConfig(\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "                max_length=max_output_length\n",
        "            )\n",
        "            fallback_config = GenerationConfig(\n",
        "                num_beams=6,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "                max_length=max_output_length\n",
        "            )\n",
        "        else:\n",
        "            print(\"Invalid model choice. Please select 'pegasus', 'bart', or 'pegasus_arxiv'.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Generate query embedding and find similar papers\n",
        "    print(\"Generating query embedding...\")\n",
        "    query_embedding = get_embedding(query, specter_tokenizer, specter_model)\n",
        "    similarities = cosine_similarity(query_embedding, paper_embeddings)\n",
        "    sorted_indices = np.argsort(similarities[0])[::-1]\n",
        "    top_papers = [all_papers[i] for i in sorted_indices]\n",
        "\n",
        "    print(f\"Found {len(top_papers)} papers, processing top 5...\")\n",
        "    # Display top 5 paper titles and links\n",
        "    print(\"\\nTop 5 relevant papers:\")\n",
        "    for i, paper in enumerate(top_papers[:5]):\n",
        "        print(f\"{i+1}. {paper['title']}\")\n",
        "        print(f\"   {paper['link']}\\n\")\n",
        "\n",
        "    # Process top papers\n",
        "    print(\"Extracting relevant content from papers...\")\n",
        "    extraction_results = process_top_papers(query, top_papers, max_papers=5, context_size=2)\n",
        "\n",
        "    # Concatenate relevant sections\n",
        "    combined_text = []\n",
        "    for paper in extraction_results:\n",
        "        for section in paper['extracted_content']['relevant_sections']:\n",
        "            for sentence in section['sentences']:\n",
        "                combined_text.append(sentence['text'])\n",
        "\n",
        "    input_text = \" \".join(combined_text).strip()\n",
        "    if not input_text:\n",
        "        print(\"No relevant content extracted for the query.\")\n",
        "        return None\n",
        "\n",
        "    # Generate summary\n",
        "    print(\"Generating summary...\")\n",
        "    try:\n",
        "        print(f\"Input text length: {len(input_text)} characters\")\n",
        "        inputs = tokenizer(input_text, max_length=max_input_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        print(\"Tokenization successful\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during tokenization: {e}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Generate summary with model-specific parameters\n",
        "        summary_ids = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during initial generation: {e}\")\n",
        "        print(\"Attempting fallback generation...\")\n",
        "        try:\n",
        "            summary_ids = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                generation_config=fallback_config\n",
        "            )\n",
        "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        except Exception as e2:\n",
        "            print(f\"Error during fallback generation: {e2}\")\n",
        "            return None\n",
        "\n",
        "    # Post-process the summary to clean up any remaining artifacts\n",
        "    cleaned_summary = postprocess_summary(summary)\n",
        "\n",
        "    # Check if the summary is too short or contains artifacts\n",
        "    if len(cleaned_summary.split()) < 20 or any(char in cleaned_summary for char in '/=+-*:'):\n",
        "        print(\"Initial summary is too short or contains artifacts. Attempting fallback generation...\")\n",
        "        try:\n",
        "            summary_ids = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                generation_config=fallback_config\n",
        "            )\n",
        "            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "            cleaned_summary = postprocess_summary(summary)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during fallback generation: {e}\")\n",
        "            return None\n",
        "\n",
        "    return cleaned_summary\n",
        "\n",
        "def main():\n",
        "    print(\"Scientific Paper Query and Summarization System\")\n",
        "    print(\"==============================================\")\n",
        "\n",
        "    # Import needed libraries if not already imported\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully!\")\n",
        "    except ImportError:\n",
        "        print(\"Not running in Google Colab or drive already mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "        return\n",
        "\n",
        "    # Get query from user\n",
        "    query = input(\"\\nEnter your research query: \")\n",
        "    if not query.strip():\n",
        "        print(\"Empty query. Please enter a valid query.\")\n",
        "        return\n",
        "\n",
        "    # Get model choice from user\n",
        "    print(\"\\nAvailable models: Pegasus, BART, Pegasus_arxiv\")\n",
        "    model_choice = input(\"Enter the model to use (Pegasus, BART, or Pegasus_arxiv): \")\n",
        "    if model_choice.lower() not in ['pegasus', 'bart', 'pegasus_arxiv']:\n",
        "        print(\"Invalid model choice. Please select 'Pegasus', 'BART', or 'Pegasus_arxiv'.\")\n",
        "        return\n",
        "\n",
        "    # Run inference\n",
        "    print(\"\\nProcessing your query. This may take a few minutes...\\n\")\n",
        "    summary = run_inference(query, model_choice)\n",
        "\n",
        "    if summary:\n",
        "        print(\"\\n=== SUMMARY ===\")\n",
        "        print(summary)\n",
        "    else:\n",
        "        print(\"\\nFailed to generate summary. Please check error messages above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}