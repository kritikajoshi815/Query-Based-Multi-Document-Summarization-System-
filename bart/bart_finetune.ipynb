{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"OM_4lOUTzvb_","outputId":"0c8b850b-e8f4-4b28-f782-adc6c2339ca6","executionInfo":{"status":"error","timestamp":1746659087400,"user_tz":-330,"elapsed":41549,"user":{"displayName":"Akhila Anumalla","userId":"10628517200289560052"}}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cac960092545>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import json\n","import os\n","import numpy as np\n","import torch\n","from transformers import BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModel\n","from datasets import Dataset\n","from rouge_score import rouge_scorer\n","from bert_score import score\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","import requests\n","from bs4 import BeautifulSoup\n","import PyPDF2\n","from io import BytesIO\n","import re\n","from sklearn.metrics.pairwise import cosine_similarity\n","import spacy\n","\n","# Ensure NLTK resources are downloaded\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","# Load spaCy model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# File paths\n","DRIVE_PATH = '/content/drive/MyDrive/nlp_project'\n","PAPERS_FILE = os.path.join(DRIVE_PATH, \"cached_papers.json\")\n","EMBEDDINGS_FILE = os.path.join(DRIVE_PATH, \"cached_embeddings.npy\")\n","MODEL_PATH = os.path.join(DRIVE_PATH, \"bart_finetuned_model\")\n","TEST_JSON = os.path.join(DRIVE_PATH, \"intermediate_training_batch_4.json\")  # Replace with your test JSON file\n","\n","# Provided functions (unchanged)\n","def download_arxiv_paper(arxiv_id):\n","    if \"arxiv.org\" in arxiv_id:\n","        arxiv_id = arxiv_id.split('/')[-1].split('v')[0]\n","    api_url = f\"http://export.arxiv.org/api/query?id_list={arxiv_id}\"\n","    response = requests.get(api_url)\n","    soup = BeautifulSoup(response.content, 'xml')\n","    abstract = soup.find('summary').text.strip() if soup.find('summary') else \"\"\n","    title = soup.find('title').text.strip() if soup.find('title') else \"\"\n","    pdf_url = f\"http://arxiv.org/pdf/{arxiv_id}.pdf\"\n","    response = requests.get(pdf_url)\n","    if response.status_code == 200:\n","        try:\n","            pdf_file = BytesIO(response.content)\n","            pdf_reader = PyPDF2.PdfReader(pdf_file)\n","            full_text = \"\"\n","            for page in pdf_reader.pages:\n","                full_text += page.extract_text()\n","            cleaned_text = clean_extracted_text(full_text)\n","            introduction = \"\"\n","            intro_patterns = [\n","                r\"(?i)(?:1\\.?\\s*|I\\.?\\s*)?Introduction(.*?)(?:\\n\\d\\.|\\n[A-Z]\\.|\\nII\\.)\",\n","                r\"(?i)(?:1\\.?\\s*|I\\.?\\s*)?Introduction(.*?)(?=\\n2\\.|\\nII\\.)\"\n","            ]\n","            for pattern in intro_patterns:\n","                intro_match = re.search(pattern, cleaned_text, re.DOTALL)\n","                if intro_match:\n","                    introduction = clean_extracted_text(intro_match.group(1).strip())\n","                    break\n","            conclusion = \"\"\n","            concl_patterns = [\n","                r\"(?i)(?:\\d\\.?\\s*|[IVX]+\\.?\\s*)?Conclusion[s]?(.*?)(?:\\n\\d\\.|\\n[A-Z]\\.|\\nReferences|\\n[IVX]+\\.)\",\n","                r\"(?i)(?:\\d\\.?\\s*|[IVX]+\\.?\\s*)?Discussion(?:s)?(.*?)(?:\\n\\d\\.|\\n[A-Z]\\.|\\nReferences|\\n[IVX]+\\.)\"\n","            ]\n","            for pattern in concl_patterns:\n","                concl_match = re.search(pattern, cleaned_text, re.DOTALL)\n","                if concl_match:\n","                    conclusion = clean_extracted_text(concl_match.group(1).strip())\n","                    break\n","            return {\n","                \"title\": title,\n","                \"abstract\": abstract,\n","                \"introduction\": introduction,\n","                \"conclusion\": conclusion,\n","                \"full_text\": cleaned_text\n","            }\n","        except Exception as e:\n","            print(f\"Error extracting text from PDF: {e}\")\n","            return {\"title\": title, \"abstract\": abstract, \"introduction\": \"\", \"conclusion\": \"\", \"full_text\": \"\"}\n","    return {\"title\": title, \"abstract\": abstract, \"introduction\": \"\", \"conclusion\": \"\", \"full_text\": \"\"}\n","\n","def clean_extracted_text(text):\n","    if not text:\n","        return \"\"\n","    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n","    text = re.sub(r'[a-zA-Z0-9]+[=><+\\-*/^()[\\]{}]+[a-zA-Z0-9]+|\\$[^$]*\\$|\\$\\$[^$]*\\$\\$', '', text)\n","    ref_patterns = [\n","        r'References\\s*\\n', r'REFERENCES\\s*\\n', r'Bibliography\\s*\\n',\n","        r'BIBLIOGRAPHY\\s*\\n', r'Works Cited\\s*\\n', r'REFERENCES CITED\\s*\\n'\n","    ]\n","    ref_start = len(text)\n","    for pattern in ref_patterns:\n","        matches = list(re.finditer(pattern, text))\n","        if matches:\n","            ref_start = min(ref_start, matches[-1].start())\n","    text = text[:ref_start]\n","    ligature_map = {\n","        '\\ufb01': 'fi', '\\ufb02': 'fl', '\\ufb00': 'ff', '\\ufb03': 'ffi', '\\ufb04': 'ffl',\n","        '\\u2019': \"'\", '\\u2018': \"'\", '\\u201c': '\"', '\\u201d': '\"', '\\u2014': '-', '\\u2013': '-', '\\u0003': ''\n","    }\n","    for ligature, replacement in ligature_map.items():\n","        text = text.replace(ligature, replacement)\n","    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d{2,4}[/-]\\d{1,2}[/-]\\d{1,2}\\b', '', text)\n","    text = re.sub(r'\\b\\d+\\.\\d+%?\\b|\\b\\d{2,}\\b|\\[\\d+\\]', '', text)\n","    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n","    text = re.sub(r'([a-z])([A-Z])|([a-z])([0-9])|([0-9])([a-z])', r'\\1 \\2', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'\\s+([.,;:!?)])', r'\\1', text)\n","    text = re.sub(r'([.,;:!?])([a-zA-Z])', r'\\1 \\2', text)\n","    return text.strip()\n","\n","def get_bert_embeddings(texts, model_name=\"bert-base-uncased\"):\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModel.from_pretrained(model_name).to(device)\n","    embeddings = []\n","    for text in texts:\n","        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        inputs = {key: val.to(device) for key, val in inputs.items()}\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","        embeddings.append(embedding[0])\n","    return np.array(embeddings)\n","\n","def highlight_query_relevant_sentences(text, query, top_n=10):\n","    text = clean_extracted_text(text)\n","    sentences = sent_tokenize(text)\n","    if not sentences:\n","        return []\n","    all_embeddings = get_bert_embeddings(sentences + [query])\n","    sentence_embeddings = all_embeddings[:-1]\n","    query_embedding = all_embeddings[-1]\n","    similarities = cosine_similarity(sentence_embeddings, query_embedding.reshape(1, -1)).flatten()\n","    top_indices = similarities.argsort()[-top_n:][::-1]\n","    return [(sentences[i], float(similarities[i]), i) for i in top_indices]\n","\n","def get_paragraph_context(sentences, index, context_size=2):\n","    start_idx = max(0, index - context_size)\n","    end_idx = min(len(sentences) - 1, index + context_size)\n","    context_paragraph = \" \".join(sentences[start_idx:end_idx + 1])\n","    return clean_extracted_text(context_paragraph)\n","\n","def extract_relevant_content(paper_data, query, max_sentences_per_section=10, context_size=2):\n","    result = {\n","        \"title\": paper_data[\"title\"],\n","        \"abstract\": paper_data[\"abstract\"],\n","        \"relevant_sections\": []\n","    }\n","    sections = {\n","        \"introduction\": paper_data.get(\"introduction\", \"\"),\n","        \"conclusion\": paper_data.get(\"conclusion\", \"\"),\n","        \"full_text\": paper_data.get(\"full_text\", \"\")\n","    }\n","    for section_name, section_text in sections.items():\n","        if not section_text:\n","            continue\n","        cleaned_text = clean_extracted_text(section_text)\n","        all_sentences = sent_tokenize(cleaned_text)\n","        relevant_sentences = highlight_query_relevant_sentences(cleaned_text, query, top_n=max_sentences_per_section)\n","        if relevant_sentences:\n","            processed_results = []\n","            relevant_sentences.sort(key=lambda x: x[2])\n","            processed_indices = set()\n","            for sent, score, idx in relevant_sentences:\n","                if idx in processed_indices:\n","                    continue\n","                context_paragraph = get_paragraph_context(all_sentences, idx, context_size)\n","                for i in range(max(0, idx - context_size), min(len(all_sentences), idx + context_size + 1)):\n","                    processed_indices.add(i)\n","                processed_results.append({\n","                    \"text\": context_paragraph,\n","                    \"relevance_score\": score,\n","                    \"core_sentence\": sent\n","                })\n","            result[\"relevant_sections\"].append({\n","                \"section_name\": section_name,\n","                \"sentences\": processed_results\n","            })\n","    return result\n","\n","def process_top_papers(query, top_papers, max_papers=5, context_size=2):\n","    results = []\n","    for i, paper in enumerate(top_papers[:max_papers]):\n","        paper_link = paper['link']\n","        paper_data = download_arxiv_paper(paper_link)\n","        relevant_content = extract_relevant_content(paper_data, query, context_size=context_size)\n","        results.append({\n","            \"paper_index\": i+1,\n","            \"title\": paper['title'],\n","            \"link\": paper['link'],\n","            \"extracted_content\": relevant_content\n","        })\n","    return results\n","\n","# Load cached papers and embeddings\n","if os.path.exists(PAPERS_FILE) and os.path.exists(EMBEDDINGS_FILE):\n","    with open(PAPERS_FILE, \"r\") as f:\n","        all_papers = json.load(f)\n","    paper_embeddings = np.load(EMBEDDINGS_FILE)\n","else:\n","    print(\"Cached data not found in Google Drive.\")\n","    exit()\n","\n","# Load Specter model\n","specter_tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter\")\n","specter_model = AutoModel.from_pretrained(\"allenai/specter\").to(device)\n","\n","# Load fine-tuned BART model\n","bart_tokenizer = BartTokenizer.from_pretrained(MODEL_PATH)\n","bart_model = BartForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n","\n","# Load test data\n","with open(TEST_JSON, \"r\", encoding=\"utf-8\") as f:\n","    test_data = json.load(f)\n","\n","# Process test data\n","val_predictions = []\n","val_references = []\n","\n","for item in test_data['training']:\n","    query = item['query']\n","    reference_summary = item['summary']\n","\n","    # Generate query embedding\n","    def get_embedding(text):\n","        inputs = specter_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","        inputs = {key: value.to(device) for key, value in inputs.items()}\n","        with torch.no_grad():\n","            output = specter_model(**inputs)\n","        return output.last_hidden_state.mean(dim=1).cpu().numpy()\n","\n","    query_embedding = get_embedding(query)\n","    similarities = cosine_similarity(query_embedding, paper_embeddings)\n","    sorted_indices = np.argsort(similarities[0])[::-1]\n","    top_papers = [all_papers[i] for i in sorted_indices]\n","\n","    # Process top papers\n","    extraction_results = process_top_papers(query, top_papers, max_papers=5, context_size=2)\n","\n","    # Concatenate relevant sections\n","    combined_text = []\n","    for paper in extraction_results:\n","        for section in paper['extracted_content']['relevant_sections']:\n","            for sentence in section['sentences']:\n","                combined_text.append(sentence['text'])\n","    input_text = \" \".join(combined_text).strip()\n","\n","    if not input_text:\n","        print(f\"No relevant content extracted for query: {query}\")\n","        continue\n","\n","    # Generate summary\n","    inputs = bart_tokenizer(input_text, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    summary_ids = bart_model.generate(\n","        inputs[\"input_ids\"],\n","        max_length=256,\n","        num_beams=4,\n","        early_stopping=True,\n","    )\n","    summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    val_predictions.append(summary)\n","    val_references.append(reference_summary)\n","\n","# Compute ROUGE scores\n","def compute_rouge_scores(predictions, references):\n","    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n","    rouge1, rouge2, rougeL = [], [], []\n","    for pred, ref in zip(predictions, references):\n","        scores = scorer.score(ref, pred)\n","        rouge1.append(scores['rouge1'].fmeasure)\n","        rouge2.append(scores['rouge2'].fmeasure)\n","        rougeL.append(scores['rougeL'].fmeasure)\n","    return {\n","        \"rouge1\": np.mean(rouge1),\n","        \"rouge2\": np.mean(rouge2),\n","        \"rougeL\": np.mean(rougeL)\n","    }\n","\n","# Compute ROUGE scores\n","rouge_scores = compute_rouge_scores(val_predictions, val_references)\n","print(\"\\nROUGE Scores:\")\n","print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n","print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n","print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n","\n","# Compute BERTScore\n","P, R, F1 = score(val_predictions, val_references, lang=\"en\", verbose=False)\n","print(\"\\nBERTScore:\")\n","print(f\"Precision: {P.mean().item():.4f}\")\n","print(f\"Recall: {R.mean().item():.4f}\")\n","print(f\"F1: {F1.mean().item():.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}